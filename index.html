<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Transformer Architecture: Deep Dive</title>
  <style>
    body {
      font-family: "Segoe UI", Arial, sans-serif;
      margin: 0;
      padding: 0;
      background: #f9fafc;
      color: #222;
    }

    header {
      background: linear-gradient(90deg, #002b5b, #00509e);
      color: white;
      padding: 30px 0;
      text-align: center;
    }

    h1 {
      margin: 0;
      font-size: 2em;
    }

    main {
      max-width: 900px;
      margin: 40px auto;
      padding: 0 20px;
      line-height: 1.6;
    }

    h2 {
      color: #00509e;
      border-bottom: 2px solid #00509e;
      padding-bottom: 5px;
    }

    .buttons {
      text-align: center;
      margin-top: 40px;
    }

    .btn {
      display: inline-block;
      background: #00509e;
      color: white;
      padding: 12px 25px;
      margin: 10px;
      border-radius: 8px;
      text-decoration: none;
      font-weight: 500;
      transition: background 0.3s;
    }

    .btn:hover {
      background: #013a73;
    }

    footer {
      text-align: center;
      margin-top: 60px;
      padding: 15px;
      background: #e9ecef;
      color: #555;
      font-size: 0.9em;
    }
  </style>
</head>

<body>
  <header>
    <h1>Transformer Architecture: Deep Dive</h1>
    <p>A 5-Day Hands-On Workshop</p>
  </header>

  <main>
    <section>
      <h2>About the Programme</h2>
      <p>
        Artificial Intelligence (AI) has transformed the landscape of computing through advanced deep learning architectures,
        among which the <b>Transformer</b> stands as a breakthrough innovation. The workshop <i>‚ÄúTransformer Architecture:
        Deep Dive‚Äù</i> aims to provide a comprehensive understanding of the internal mechanisms, mathematical foundations, and
        implementation aspects of transformer-based models that power modern AI systems such as <b>BERT, GPT, Vision
        Transformers</b>, and their variants.
      </p>
      <p>
        This five-day intensive workshop will take participants through theoretical and practical aspects of Transformer models ‚Äî
        from optimization concepts to the design of multi-headed attention, positional encoding, and encoder-decoder architectures.
        With a strong emphasis on <b>hands-on learning</b>, participants will gain practical skills for building and fine-tuning
        Transformer models using PyTorch and Hugging Face.
      </p>
    </section>

    <section>
      <h2>Topics Covered (but not limited to)</h2>
      <ul>
        <li><b>Foundations of Neural Optimization:</b> Gradient Descent, Backpropagation, and Loss Functions.</li>
        <li><b>Core Transformer Components:</b> Self-Attention, Multi-Head Attention, Layer Normalization, and Residual Connections.</li>
        <li><b>Encoder‚ÄìDecoder Framework:</b> Cross-Attention, Masking, and Full Model Construction.</li>
        <li><b>Advanced Architectures:</b> BERT, GPT, Vision Transformers, Longformer, Reformer, Mamba Variants.</li>
        <li><b>Hands-on Practice:</b> PyTorch implementation, Attention Visualization, and Fine-tuning models on datasets.</li>
        <li><b>Research Trends:</b> Sparse Attention, RAG Integration, and Future Transformer Directions.</li>
      </ul>
    </section>

    <div class="buttons">
      <a href="mom.pdf" class="btn" target="_blank">üìò Click for MOM</a>
      <a href="brochure.pdf" class="btn" target="_blank">üìÑ Download Brochure</a>
      <a href="https://forms.gle/exampleformlink" class="btn" target="_blank">üìù Register Now</a>
    </div>
  </main>

  <footer>
    ¬© 2025 Transformer Deep Dive Workshop | All Rights Reserved
  </footer>
</body>
</html>
